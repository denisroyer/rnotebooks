{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bf1ee5-a103-4f97-aeda-425947164cff",
   "metadata": {},
   "source": [
    "![R-Kenntnis](../Pics/header.png \"R-Kenntnis\")\n",
    "\n",
    "<img src=\"https://img.shields.io/badge/--Kenntnis-blue?style=flat-square&logo=r&logoColor=white\"/><a href=\"https://ostfalia.de/w\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Ostfalia-Fakultät%20W-blue?style=flat-square&logo=googlescholar&logoColor=white\"/></a> <img src=\"https://img.shields.io/badge/Semester-WiSe2022%2F23-green?style=flat-square\"/> <img src=\"https://img.shields.io/badge/Copyright-2020--22-orange?style=flat-square\"/> <a href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\" target=\"_blank\"><img src=\"https://img.shields.io/badge/License-by--nc--sa-red?style=flat-square\"/></a> \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "* **Titel:** R-Kenntnis Lösung Übungen 09: Klassifikation und Clustering mit R\n",
    "* **Autor:** Prof. Dr. Denis Royer\n",
    "* **Datum:** 14.12.2022 (Version 2.0)\n",
    "\n",
    "</div>\n",
    "\n",
    "# BI - R-Kenntnis Lösung Übungen 09: Klassifikation und Clustering mit R\n",
    "\n",
    "Dieses Übungsblatt bezieht sich auf den R-Kurs ***R-Kenntnis*** zur Vorlesung Business Intelligence im Wintersemester 2022/23.\n",
    "\n",
    "Bitte lesen Sie die folgenden Kapitel und die enthaltenen Hinweise ***sorgfältig*** durch. Die Aufgaben sind zum Teil in den Kapiteln enthalten.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>WICHTIG:</b> \n",
    "    \n",
    "*Die Schritte im Source Code bauen zum Teil aufeinander auf. Sollten Sie Ihre RStudio oder JupyterLab Session schließen oder neu starten, so müssen Sie ggf. den **Code erneut ausführen**, bzw. die **Packages neu laden**.*\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Hinweis:</b> Weitere Hinweise und Quellen finden Sie <a href=\"../index.ipynb\">auf der zenteralen Übersicht zu den Übungen</a>. </div>\n",
    "\n",
    "## Vorbereitende Schritte für die Übung 09\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Wichtig:</b> Bevor wir loslegen, müssen wir zunächst einmal ein paar vorbereitende Dinge erledigen:\n",
    "\n",
    "* Die notwendigen Packages laden (bspw. `tidyverse`)\n",
    "* Die übrigen Packages werden nach und nach in den Teilübungen geladen.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ba177",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "name": "prepare_env",
    "tags": [
     "remove_cell"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(knitr)\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(caret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83d23e",
   "metadata": {},
   "source": [
    "# Klassifikation und Clustering mit R\n",
    "\n",
    "In dieser Übung geht es um die Anwendung von Methoden zur Klassifikation und Clustering in ***R***. Als Datensatz für die Demonstration von Klassifikation und Clustering wird das Dataset `iris` benutz. \n",
    "\n",
    "Beim Iris Datensatz, handelt es sich um einen Datensatz mit 150 Beobachtungen von 4 Attributen von Schwertlilien. Gemessen wurden dabei jeweils die Breite und die Länge des Kelchblatts (Sepalum) sowie des Kronblatts (Petalum) in Zentimeter. Des Weiteren ist für jeden Datensatz die Art der Schwertlilie (Iris setosa, Iris virginica oder Iris versicolor) angegeben. Für jede Schwertlilienart liegen 50 Datensätze vor. \n",
    "\n",
    "![](../Pics/iris-machinelearning.png)\n",
    "\n",
    "Dieser Datensatz wird - wie auch hier - in der Clusteranalyse bzw. in der Mustererkennung als Testdatensatz herangezogen, um aufgrund der gemessenen Attribute die Art der Schwertlilie automatisch zu erkennen. Der Datensatz eignet sich gerade deshalb so gut, weil eine Schwertlilienart aufgrund der Messungen sehr gut identifiziert werden kann, während die beiden anderen Arten überschneidende Wertebereiche in jeder Eigenschaft aufweisen. Außerdem ist eine Darstellung aller Informationen im dreidimensionalen Datenraum (z. B. mit einem 3D-Scatterplot) nicht mehr möglich, weshalb der Datensatz auch in der Informationsvisualisierung, die Daten ohne inhärenten Raumbezug analysiert, zur Anwendung kommt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d882f1d0",
   "metadata": {
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Ausgabe der ersten Zeilen des iris Datensatzes\n",
    "data(\"iris\")\n",
    "head(iris) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c9646-8658-4841-8894-808b1d7aec09",
   "metadata": {},
   "source": [
    "## Grafische Exploration der Daten\n",
    "\n",
    "Im Folgenden schauen wir uns die iris Daten einmal genauer an. Ein erster Ansatz in Richtung der Klassifikation und des Clustering ist es, sich die Daten und deren Verteilung pro Klasse einmal genauer anzuschauen. Im Beispiel geschieht dies für das Datum *Sepal.Width*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8231a4c",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "library(psych)\n",
    "\n",
    "pairs.panels(iris[1:4],\n",
    "             gap=0,\n",
    "             bg = c(\"red\",\"green\",\"blue\")[iris$Species],\n",
    "             pch=21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253c96f-73a8-437e-9454-edf95e9681d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plot1 <- ggplot(data=iris, aes(Sepal.Length, fill = Species))+ \n",
    "  theme_bw()+\n",
    "  geom_density(alpha=0.25)+\n",
    "  labs(x = \"Sepal.Length\", title=\"Spezies vs. Sepal Length\")\n",
    "\n",
    "\n",
    "plot2 <- ggplot(data=iris, aes(x = Sepal.Width)) +\n",
    " stat_density(aes(ymax = ..density..,  ymin = -..density.., \n",
    "                       fill = Species, color = Species), \n",
    "                   geom = \"ribbon\", position = \"identity\") +\n",
    "  facet_grid(. ~ Species) + coord_flip() + theme_bw()+labs(x = \"Sepal Width\", title=\"Spezies vs. Sepal Width\")\n",
    "\n",
    "grid.arrange(plot1, plot2, ncol=2)\n",
    "\n",
    "# Mehr zum Thema Anordnen von plots und ggplot2 finden Sie hier: \n",
    "# https://stackoverflow.com/questions/1249548/side-by-side-plots-with-ggplot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42d11d-89c4-406c-a918-07223cab9911",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Mehr zum Thema Visualisierung (iris und ggplot2) finden Sie hier:</b> \n",
    "\n",
    "* <https://www.kaggle.com/leolcling/visualizing-iris-datasets-with-r-ggplot2>\n",
    "* <https://hub.packtpub.com/data-visualization-ggplot2/>\n",
    "* <https://www.datanovia.com/en/lessons/introduction-to-ggplot2/>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34743036",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "\n",
    "Zunächst schauen wir uns den Bereich Klassifikation an - spezifisch die folgenden Aspekte:\n",
    "\n",
    "* Erstellen von Trainings- und Testdatensätzen\n",
    "* Recursive Partytioning (`party` - Package)\n",
    "* Naive Baysian (`caret` - Package)\n",
    "* k-nächste-Nachbarn (`class` - Package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3cae6-6427-4204-8412-0ce0b05a4e2d",
   "metadata": {},
   "source": [
    "![Übungsaufgabe](../Pics/excercise.png \"Übungsaufgabe\")\n",
    "***Aufgaben/Fragen:***\n",
    "\n",
    "* Für die Klassifikation: Was sind die Werte für X, C, O, etc. (Klassifikationsfunktion $O(x_1, \\dots , x_d) - f: X \\rightarrow C$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c117b3e-0758-4862-8723-0897d2660cdf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Lösungsansatz:</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e94a6-0e2c-4c24-8d1a-d5e7ec6be615",
   "metadata": {},
   "source": [
    "**Grundsätzlich gilt:** Gegeben ist eine Menge $O$ von Objekten des Formats ($x_1, \\dots, x_d$) mit Attributen $X_i$, $1≤ i≤ d$, und Klassenzugehörigkeit $c_i, c_i \\in C = {c_1, \\dots , c_k}$ \n",
    "\n",
    "Damit ergeben sich folgende Ausprägungen von X, C und O:\n",
    "\n",
    "* **X:** Die Attribute *Sepal.Length, Sepal.Width, Petal.Length, Petal.Width*\n",
    "* **C:** Spezies Zuordnung der Objekte: *setosa versicolor virginica*\n",
    "* **O:** Die Daten im Datensatz iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1346d7-3f68-446a-80d7-754f7fb6df3e",
   "metadata": {},
   "source": [
    "### Erstellen  von Trainings- und Testdatensätzen\n",
    "\n",
    "Trainings- und Testdatensätze sind zwei verschiedene Teile eines Datensatzes, die beim maschinellen Lernen verwendet werden. Der Trainingsdatensatz wird verwendet, um ein maschinelles Lernmodell zu erstellen, indem es auf die Daten im Trainingsdatensatz trainiert wird. Der Testdatensatz wird verwendet, um das Modell zu bewerten, indem es auf die Daten im Testdatensatz angewendet wird.\n",
    "\n",
    "Der Hauptunterschied zwischen Trainings- und Testdatensätzen besteht darin, dass der Trainingsdatensatz verwendet wird, um das Modell zu erstellen, während der Testdatensatz verwendet wird, um das Modell zu bewerten. Der Trainingsdatensatz sollte daher so ausgewählt werden, dass er dem Modell genügend Informationen liefert, um es zu trainieren, ohne dabei zu sehr von dem Testdatensatz abzugrenzen. Der Testdatensatz sollte hingegen so ausgewählt werden, dass er dem Modell genügend Herausforderungen stellt, um die Genauigkeit des Modells zu bewerten, ohne dabei zu sehr vom Trainingsdatensatz abzugrenzen.\n",
    "\n",
    "Auf folgende Weise lassen sich Trainings- und Testdatensätzen aus dem `iris` Datensatz erzeugen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95236c-cfc9-4bcb-a7a7-df9ab6817f78",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Daten in 2 Subsets aufteilen: \n",
    "# - Trainingsdaten (70%) und \n",
    "# - Testdaten (30%); \n",
    "\n",
    "# Zufallsvariable so setzen, damit Ergebnisse reproduzierbar werden\n",
    "set.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418122da-8797-4719-ad7b-f3d19f207bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample Funktion nutzen, um Trainings- und Testdatensatz zu erzeugen .\n",
    "#\n",
    "# Mit sample() können von einem Vektor Teilstichproben einer festgelegten \n",
    "# Länge mit und ohne Zurücklegen realsiert werden. Dies ist zum Beispiel \n",
    "# für Bootstrapping-Verfahren (Resampling) notwendig.\n",
    "\n",
    "index <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))\n",
    "\n",
    "# Sample erzeugt einen neuen Vector \"index\" mit der Länge des\n",
    "# Datensatzes \"iris\". Die Zahlen im Vector \"index\" entsprechen\n",
    "# jeweils der Zuordnung zu den Trainings- und Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d39349-6ed5-40c8-92b5-5f17036f21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index\n",
    "index[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372058f-b4da-4994-b97d-af6764cd65e8",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Aufteilung der Daten mit der Hilfe der Variable \"index\"\n",
    "# 70% Trainingsdaten --> Index == 1\n",
    "# 30% Testdaten --> Index == 2\n",
    "train.data <- iris[index == 1, ]\n",
    "test.data <- iris[index == 2, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2319a1-a677-4347-9d30-5ab4e8e8f3bd",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Eine Confusion Matrix ist ein Werkzeug zur Bewertung der Leistung eines Klassifikationsmodells. Sie zeigt die Anzahl der korrekt und falsch klassifizierten Datenpunkte in einem Klassifikationsproblem. Die Confusion Matrix enthält verschiedene Kennzahlen, die verwendet werden können, um die Leistung des Modells zu bewerten.\n",
    "\n",
    "Die Confusion Matrix enthält im Allgemeinen vier Hauptkennzahlen: \n",
    "\n",
    " * Die Anzahl der true positives (korrekt klassifizierte Datenpunkte, die tatsächlich in der positiven Klasse liegen)\n",
    " * Die Anzahl der false positives (falsch klassifizierte Datenpunkte, die tatsächlich in der negativen Klasse liegen)\n",
    " * Die Anzahl der true negatives (korrekt klassifizierte Datenpunkte, die tatsächlich in der negativen Klasse liegen) \n",
    " * Die Anzahl der false negatives (falsch klassifizierte Datenpunkte, die tatsächlich in der positiven Klasse liegen).\n",
    "\n",
    "Diese Kennzahlen können verwendet werden, um verschiedene Metriken zu berechnen, die die Leistung des Klassifikationsmodells bewerten, z.B. die Genauigkeit, die Sensitivität, die Spezifität und der F1-Score. Die Genauigkeit gibt an, wie viele der klassifizierten Datenpunkte insgesamt korrekt sind, während die Sensitivität und die Spezifität die Fähigkeit des Modells angeben, die positiven und negativen Klassen korrekt zu klassifizieren. Der F1-Score ist eine Kombination aus Sensitivität und Spezifität und gibt die allgemeine Leistung des Modells an.\n",
    "\n",
    "![](../Pics/confusionmatrix.jpg)\n",
    "\n",
    "In diesem Beispiel wird angenommen, dass das Klassifikationsmodell die Klasse \"Positive\" vorhersagt. Die Confusion Matrix zeigt die Anzahl der korrekt und falsch klassifizierten Datenpunkte in der Klasse \"Positive\" (RP und FP) und in der Klasse \"Negative\" (FN und RN). Aus dieser Confusion Matrix können verschiedene Metriken berechnet werden, um die Leistung des Modells zu bewerten. Zum Beispiel kann die Genauigkeit wie folgt berechnet werden:\n",
    "\n",
    "Genauigkeit = (RP + RN) / (RP + RN + FP + FN)\n",
    "\n",
    "Die Genauigkeit gibt an, wie viele der klassifizierten Datenpunkte insgesamt korrekt sind. In diesem Beispiel würde die Genauigkeit das Verhältnis der korrekt klassifizierten Datenpunkte zu allen klassifizierten Datenpunkten darstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28444c01",
   "metadata": {},
   "source": [
    "### Recursive Partytioning Klassifikation (`party` - Package)\n",
    "\n",
    "Recursive Partytioning Klassifikation ist ein Verfahren zum Klassifizieren von Daten, bei dem ein Klassifikationsbaum verwendet wird, um die Datenpunkte in verschiedene Klassen einzuteilen. Der Klassifikationsbaum wird dabei durch rekursive Aufteilung der Daten in immer kleinere Teilmengen erstellt, bis jeder Datenpunkt in seiner eigenen Teilmenge enthalten ist.\n",
    "\n",
    "Das Verfahren beginnt damit, dass alle Datenpunkte in einer einzigen Teilmenge zusammengefasst werden. Anschließend werden die Datenpunkte auf der Grundlage eines bestimmten Kriteriums in zwei Teilmengen aufgeteilt. Dieser Schritt wird wiederholt, bis jeder Datenpunkt in seiner eigenen Teilmenge enthalten ist. Die Entscheidung, wie die Datenpunkte in Teilmengen aufgeteilt werden, wird anhand eines bestimmten Kriteriums getroffen, das normalerweise auf dem Verhältnis von Klassen in den Teilmengen basiert.\n",
    "\n",
    "Ein Vorteil von Recursive Partytioning Klassifikation ist, dass es ein einfaches und effektives Verfahren ist, das gut für große Datensätze geeignet ist. Ein Nachteil ist jedoch, dass es leicht zu Overfitting neigt, insbesondere bei komplexen Klassifikationsproblemen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d61ea5-676b-4919-8cff-0b399041ddc5",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Klassifikation:\n",
    "#### Recursive Partytioning (party)\n",
    "#### Entscheidungsbaum erstellen\n",
    "##########################################################\n",
    "\n",
    "library(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92e831-5a4c-4504-9270-7a890a6518d8",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Erstellung eines Conditional Inference Trees\n",
    "# Recursive Partytioning\n",
    "myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\n",
    "iris.ctree <- ctree(myFormula, data = train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb40c76",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# check the prediction\n",
    "table(predict(iris.ctree), train.data$Species)\n",
    "print(iris.ctree)\n",
    "plot(iris.ctree)\n",
    "plot(iris.ctree, type = \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac90f26-fae7-48ea-a54a-c923cc5f106a",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#### Modell überprüfen\n",
    "\n",
    "# Vorhersage testen\n",
    "testPred <- predict(iris.ctree, newdata = test.data)\n",
    "tab.dtree <- table(testPred, test.data$Species)\n",
    "# Anzahl richtiger Entscheidungen\n",
    "class.right <- sum(diag(tab.dtree))\n",
    "# Anzahl falscher Entscheidungen\n",
    "class.table<- sum(tab.dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621522e3-b2e1-46da-abd7-9a1f62388ce6",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tab.dtree\n",
    "precision.dtree <- class.right/(class.table)\n",
    "precision.dtree * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd813d3-2f90-4b52-a652-1e1673dd5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wie gut ist die Vorhersagekraft?\n",
    "confusionMatrix(data=testPred, reference = test.data$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26269df",
   "metadata": {},
   "source": [
    "### Naive Baysian Klassifikation (`caret` - Package)\n",
    "\n",
    "Ein Bayes-Klassifikator, ist ein aus dem Satz von Bayes hergeleiteter Klassifikator. Er ordnet jedes Objekt der Klasse zu, zu der es mit der größten Wahrscheinlichkeit gehört, oder bei der durch die Einordnung die wenigsten Kosten entstehen. Formal handelt es sich um eine mathematische Funktion, die jedem Punkt eines Merkmalsraums eine Klasse zuordnet.\n",
    "\n",
    "Um den Bayes-Klassifikator zu definieren, wird ein Kostenmaß benötigt, das jeder möglichen Klassifizierung Kosten zuweist. Der Bayes-Klassifikator ist genau derjenige Klassifikator, der die durch alle Klassifizierungen entstehenden Kosten minimiert. Das Kostenmaß wird gelegentlich auch Risikofunktion genannt; man sagt dann, der Bayes-Klassifikator minimiere das Risiko einer Fehlentscheidung und sei über das *minimum-risk*-Kriterium definiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edec84a-005d-4759-848b-c59e64ec9640",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Klassifikation:\n",
    "#### Naive Baysian (caret)\n",
    "##########################################################\n",
    "\n",
    "library(caret)\n",
    "library(psych)\n",
    "library(klaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16690232-bb3d-4ed5-9688-864b39160d58",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "iris.caret <- iris\n",
    "\n",
    "iris.caret.features = iris.caret[,-5]\n",
    "irisiris.caret.classes = iris.caret$Species\n",
    "\n",
    "iris.caret.fit = train(iris.caret.features,\n",
    "                       irisiris.caret.classes,\n",
    "                       'nb',\n",
    "                       trControl=trainControl(method='cv',number=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f8833",
   "metadata": {
    "eval": false,
    "fig.height": 3,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "iris.caret.fit\n",
    "\n",
    "predict(iris.caret.fit$finalModel,iris.caret.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b617067-3fc7-4640-81f4-1755eab1d637",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tab.caret <- table(predict(iris.caret.fit$finalModel,iris.caret.features)$class,irisiris.caret.classes)\n",
    "# Anzahl richtiger Entscheidungen\n",
    "class.right <- sum(diag(tab.caret))\n",
    "# Anzahl falscher Entscheidungen\n",
    "class.table<- sum(tab.caret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bc677-2d42-4d8f-9200-22e22b76b417",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Wie gut ist die Vorhersagekraft?\n",
    "precision.caret <-class.right/(class.table) # --> 96%\n",
    "precision.caret * 100\n",
    "\n",
    "naive_iris <- NaiveBayes(iris$Species ~ ., data = iris)\n",
    "plot(naive_iris )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd95a79",
   "metadata": {},
   "source": [
    "### k-nächste-Nachbarn Klassifikation (`class` - Package)\n",
    "\n",
    "Die Nächste-Nachbarn-Klassifikation ist eine parameterfreie Methode zur Schätzung von Wahrscheinlichkeitsdichtefunktionen. Der daraus resultierende k-Nearest-Neighbor-Algorithmus (KNN, zu Deutsch \"k-nächste-Nachbarn-Algorithmus\") ist ein Klassifikationsverfahren, bei dem eine Klassenzuordnung unter Berücksichtigung seiner k nächsten Nachbarn vorgenommen wird. Der Teil des Lernens besteht aus simplem Abspeichern der Trainingsbeispiele, was auch als lazy learning (\"träges Lernen\") bezeichnet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b94ed-cbf8-4d57-9341-a320321015d1",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Klassifikation:\n",
    "#### k-Nearest-Neighbor (KNN)\n",
    "##########################################################\n",
    "\n",
    "library(class)\n",
    "iris.knn.fit <- knn(train= train.data[,-5],test=test.data[,-5], cl= train.data[,5],k=13)\n",
    "table(factor(iris.knn.fit))\n",
    "\n",
    "iris.knn.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3678b5-4be6-4c8c-b2fc-3eaa053ee1b1",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "#### Modell überprüfen\n",
    "\n",
    "# Vorhersage testen\n",
    "testPred <- predict(iris.ctree, newdata = test.data)\n",
    "tab.knn <- table(test.data[,5],iris.knn.fit)\n",
    "# Anzahl richtiger Entscheidungen\n",
    "class.right <- sum(diag(tab.knn))\n",
    "# Anzahl falscher Entscheidungen\n",
    "class.table<- sum(tab.knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72674d26-cbba-49ce-be12-b95e8bcc9759",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "# Wie gut ist die Vorhersagekraft?\n",
    "precision.knn <- class.right/(class.table)\n",
    "precision.knn * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa92f9",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Im nächsten Schritt geht es darum, die Daten aus `iris` automatisiert in Cluster zu teilen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ad5e9",
   "metadata": {
    "fig.height": 3,
    "lines_to_next_cell": 2,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "# Ausgabe Originaldaten\n",
    "plot(iris[,1:4], col = iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb37436",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clustering mit k-means\n",
    "\n",
    "Clustering mit k-means ist ein Verfahren zum Gruppieren von Daten in Cluster. Das Ziel des k-means-Clustering ist es, die Daten so zu gruppieren, dass die Datenpunkte innerhalb eines Clusters möglichst ähnlich sind, während die Datenpunkte in verschiedenen Clustern möglichst unterschiedlich sind.\n",
    "\n",
    "Das k-means-Verfahren beginnt damit, dass zunächst zufällig k-Cluster ausgewählt werden. Jeder Cluster wird durch seinen Mittelwert, auch als Centroid bezeichnet, repräsentiert. Anschließend werden die Datenpunkte den Clustern zugeordnet, indem jedem Datenpunkt der Cluster zugewiesen wird, dessen Centroid ihm am nächsten ist. Sobald die Zuordnungen feststehen, werden die Centroids neu berechnet, indem der Mittelwert aller Datenpunkte in jedem Cluster berechnet wird. Dieser Schritt wird wiederholt, bis die Centroids sich nicht mehr verändern oder eine vorher festgelegte Anzahl von Iterationen erreicht wurde.\n",
    "\n",
    "Insgesamt ist k-means-Clustering ein einfaches und effektives Verfahren zum Gruppieren von Daten, das jedoch auch seine Grenzen hat. Ein wichtiger Nachteil des k-means-Verfahrens ist, dass es davon ausgeht, dass die Cluster kugelförmig sind und dass alle Cluster die gleiche Varianz haben. In der Praxis können die Daten jedoch in Clustern mit unterschiedlicher Form und Varianz vorkommen, was dazu führen kann, dass das k-means-Verfahren nicht immer die besten Ergebnisse liefert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac3475-9e8b-4cf9-93ac-350cbeda60f0",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Clustering:\n",
    "#### Bsp.: k-means\n",
    "##########################################################\n",
    "\n",
    "# Daten laden\n",
    "iris.clustering <- iris\n",
    "\n",
    "# nicht so wichtig für das iris Dataset (metrisch Skaliert)\n",
    "# Aber bei metrischen Skalen wichtig (Alter, Abstand)\n",
    "# Weiterhin: Nur kontinuierliche Daten sollen sakliert werden\n",
    "# iris.clustering.scaled <- scale(iris.clustering[,-5])\n",
    "\n",
    "# Es gibt verschiedene Algorithmen\n",
    "# k-means wird hier genutzt (Hierarchisches Clustering)\n",
    "\n",
    "\n",
    "fit.K <- kmeans(iris.clustering[,-5], 3)\n",
    "fit.K\n",
    "\n",
    "plot(iris, col = fit.K$cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d186de1-dd91-4a88-bc88-e5e6b16d4b86",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "# bessere Annährung von k suchen\n",
    "# Hierfür prüfn wir einfach die Parameter für k\n",
    "# von 1 bis 10 einmal durch - dies geschieht mit einer \n",
    "# for-Schleife\n",
    "\n",
    "k <- list()\n",
    "for (i in 1:10){\n",
    "  k[[i]] <- kmeans(iris.clustering[,-5], i)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc01a6-70ce-46af-be78-1948ec79d648",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "# Als nächstes prüfen wir, wie gut die Abdeckung der Cluster ist\n",
    "# und geben das Ergebnis als Plot einmal aus\n",
    "\n",
    "k.betweenss_toss <- list()\n",
    "\n",
    "for(i in 1:10){\n",
    "  k.betweenss_toss[[i]] <- k[[i]]$betweenss/k[[i]]$totss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0dccd0-7d8b-4ea2-b17c-03ac9a068086",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "plot(1:10, \n",
    "     k.betweenss_toss, \n",
    "     type = \"b\",\n",
    "     ylab=\"Between SS / Total SS\",\n",
    "     xlab=\"Clusters (k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79870f77-b42f-4f75-890d-01603c1999bf",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "par(mfrow = c(2, 2))\n",
    "# Analyse der Cluster für k = 1-4\n",
    "for(i in 1:4){\n",
    "  plot(iris, \n",
    "       col = k[[i]]$cluster,\n",
    "       main = paste (\"Plot für k=\", i))\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b5d0c-9ba2-44d5-9871-ea69c3e62879",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "k <- list()\n",
    "for (i in 1:10){\n",
    "  k[[i]] <- kmeans(iris[,-5], i)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41861e05-9568-413c-a014-b6b0573b942e",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# Als nächstes prüfen wir, wie gut die Abdeckung der Cluster ist\n",
    "# und geben das Ergebnis als Plot einmal aus\n",
    "\n",
    "k.betweenss_toss <- list()\n",
    "\n",
    "for(i in 1:10){\n",
    "  k.betweenss_toss[[i]] <- k[[i]]$betweenss/k[[i]]$totss\n",
    "}\n",
    "\n",
    "plot(1:10, \n",
    "     k.betweenss_toss, \n",
    "     type = \"b\",\n",
    "     ylab=\"Between SS / Total SS\",\n",
    "     xlab=\"Clusters (k)\",\n",
    "     main = \"Festlegung von k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097cca03",
   "metadata": {},
   "source": [
    "### Hierarchisches Clustering\n",
    "\n",
    "Hierarchisches Clustering ist ein Verfahren zum Gruppieren von Daten, bei dem die Datenpunkte zunächst alle in eigene Cluster eingeordnet werden und anschließend sukzessive zu immer größeren Clustern zusammengefasst werden. Dieses Verfahren kann dazu verwendet werden, um die Struktur der Daten zu entdecken und die Beziehungen zwischen den Datenpunkten aufzudecken.\n",
    "\n",
    "Es gibt zwei Haupttypen hierarchischen Clustering: agglomeratives Clustering und divisives Clustering. Beim agglomerativen Clustering werden die Datenpunkte zunächst in eigene Cluster eingeordnet und anschließend sukzessive zusammengefasst, bis alle Datenpunkte in einem einzigen Cluster sind. Beim divisiven Clustering werden die Datenpunkte zunächst in einem einzigen Cluster zusammengefasst und anschließend sukzessive in immer kleinere Cluster aufgeteilt.\n",
    "\n",
    "Ein wichtiger Vorteil von hierarchischem Clustering ist, dass es keine vorherige Angabe der Anzahl der Cluster erfordert, wie dies beim k-means-Clustering der Fall ist. Stattdessen wird die Anzahl der Cluster aus den Daten selbst abgeleitet, was es zu einem flexiblen Verfahren macht. Ein Nachteil von hierarchischem Clustering ist jedoch, dass es in der Regel langsamer ist als andere Clustering-Verfahren, insbesondere bei großen Datensätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de7732-53fc-4535-be43-87e6f955b3b2",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Clustering:\n",
    "#### Bsp.: Hierarchisches Clustering\n",
    "##########################################################\n",
    "\n",
    "d <- dist(iris.clustering[,-5])\n",
    "# Parameter --> Typ von Clustering\n",
    "fit.H <- hclust(d, \"ward.D2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330770b-092e-4568-973a-ba8be14a26c2",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Dendrogramm ausgeben\n",
    "plot(fit.H,\n",
    "     main = \"Dendrogramm\")\n",
    "rect.hclust(fit.H, k=3, border = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc35b3-9368-4e4f-b906-65b8acf8a16a",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Um die Zugehörigkeit eines Datensatzes zu einem Cluster \n",
    "# zu finden, müssen diese einmal mit cutree() in eine Variable\n",
    "# Ausgelesen werden\n",
    "\n",
    "clusters.H <- cutree(fit.H, 3)\n",
    "clusters.H\n",
    "\n",
    "plot(iris, col = clusters.H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c06b1de",
   "metadata": {},
   "source": [
    "### Weiter Clustering Methoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9c470-c5d1-435c-8d91-9ac659731a0e",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Clustering:\n",
    "#### Bsp.: Model-based Clustering\n",
    "##########################################################\n",
    "\n",
    "library(mclust)\n",
    "fit.M <- Mclust(iris.clustering[,-5])\n",
    "fit.M\n",
    "\n",
    "plot(fit.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622bbf2-15f9-453f-8645-26d6d4d72573",
   "metadata": {
    "eval": false,
    "include": true,
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "#### Clustering:\n",
    "#### Bsp.: Density-based Clustering\n",
    "##########################################################\n",
    "\n",
    "library(dbscan)\n",
    "\n",
    "kNNdistplot(iris.clustering[,-5], k=3)\n",
    "abline(h = 0.7, col = \"red\", lty = 2)\n",
    "# minPts --> Anzahl Variablen + 1 ==> 5\n",
    "fit.D <- dbscan(iris.clustering[,-5], eps = 0.7, minPts = 5)\n",
    "fit.D\n",
    "plot(iris, col = fit.D$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5de024-8a8f-4a03-99a0-62033821765e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lösungen\n",
    "\n",
    "Die Lösungen sind in diesem Arbeitsblatt schon direkt enthalten."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "message,eval,fig.height,include,name,tags,warning,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
